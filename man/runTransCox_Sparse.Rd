% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/runTransCox_Sparse.R
\name{runTransCox_Sparse}
\alias{runTransCox_Sparse}
\title{Sparse TransCox Model for High-Dimensional Survival Analysis}
\usage{
runTransCox_Sparse(
  primData,
  auxData,
  cov = c("X1", "X2"),
  statusvar = "status",
  lambda1 = NULL,
  lambda2 = NULL,
  lambda_beta = NULL,
  learning_rate = 0.001,
  nsteps = 500,
  auto_tune = TRUE,
  use_sparse = NULL,
  verbose = TRUE,
  tolerance = 1e-06,
  early_stopping = TRUE,
  adaptive_lr = TRUE,
  parallel = FALSE,
  n_cores = NULL,
  threshold_c = 0.5
)
}
\arguments{
\item{primData}{A data.frame containing the target domain survival data. Must include
survival time, event status, and covariates.}

\item{auxData}{A data.frame containing the source domain survival data with the same
structure as primData.}

\item{cov}{A character vector specifying the names of covariates to be used in the model.
Default is c("X1", "X2").}

\item{statusvar}{A character string specifying the name of the event status variable.
Default is "status".}

\item{lambda1}{Numeric. L1 penalty parameter for eta (auxiliary parameter). If NULL,
will be automatically selected via BIC. Default is NULL.}

\item{lambda2}{Numeric. L1 penalty parameter for xi (transfer parameter). If NULL,
will be automatically selected via BIC. Default is NULL.}

\item{lambda_beta}{Numeric. L1 penalty parameter for beta_t (target parameter). If NULL,
will be automatically selected via BIC. Default is NULL.}

\item{learning_rate}{Numeric. Learning rate for the optimization algorithm. Default is 0.004.}

\item{nsteps}{Integer. Maximum number of optimization steps. Default is 200.}

\item{auto_tune}{Logical. Whether to automatically tune hyperparameters using BIC.
Default is TRUE.}

\item{use_sparse}{Logical. Whether to force the use of sparse implementation. If NULL,
automatically determined based on data dimensions. Default is NULL.}

\item{verbose}{Logical. Whether to display detailed progress information. Default is TRUE.}

\item{tolerance}{Numeric. Convergence tolerance for the optimization algorithm.
Default is 1e-6.}

\item{early_stopping}{Logical. Whether to enable early stopping mechanism. Default is TRUE.}

\item{adaptive_lr}{Logical. Whether to use adaptive learning rate. Default is TRUE.}

\item{parallel}{Logical. Whether to use parallel computation for parameter search. Default is FALSE.}

\item{n_cores}{Integer. Number of cores for parallel computation. If NULL, detected automatically.}

\item{threshold_c}{Numeric. Constant for theoretical hard thresholding (tau = C * sqrt(log(p)/n)).
Default is 0.5. Controls the post-hoc sparsity level.}
}
\value{
A list containing the following components:
\describe{
\item{beta_t}{Estimated coefficients for the target domain}
\item{eta}{Estimated auxiliary parameters}
\item{xi}{Estimated transfer parameters}
\item{lambda1}{Used L1 penalty for eta}
\item{lambda2}{Used L1 penalty for xi}
\item{lambda_beta}{Used L1 penalty for beta_t}
\item{convergence}{Convergence information}
\item{sparse_info}{Information about sparsity patterns}
}
}
\description{
This function implements the sparse TransCox model for high-dimensional survival analysis
with transfer learning. It integrates source domain (auxiliary) data to improve prediction
performance on the target domain (primary) data through regularized Cox regression with
L1 penalties.
}
\details{
The sparse TransCox model addresses the challenge of limited sample sizes in survival
analysis by leveraging information from related source domains. The model uses L1
regularization to achieve sparsity and automatic feature selection, making it suitable
for high-dimensional data where the number of features exceeds the number of samples.
}
\examples{
\dontrun{
# Generate example data
data <- generate_sparse_survival_data(n_main = 100, n_aux = 200, p = 50)

# Fit sparse TransCox model
result <- runTransCox_Sparse(
   primData = data$prim_data,
   auxData = data$aux_data,
   cov = paste0("X", 1:50),
   statusvar = "status"
)

# View results
print(result)
}

}
